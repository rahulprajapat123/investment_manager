# Portfolio Analytics Platform - Technical Documentation

## Executive Summary

A comprehensive portfolio analytics platform built with **Flask** backend and **React** frontend that ingests multi-broker investment data, normalizes it into canonical schemas, validates data quality, performs aggregations, and generates detailed Excel reports for multiple clients.

---

## Table of Contents

1. [Project Architecture](#project-architecture)
2. [Data Pipeline Overview](#data-pipeline-overview)
3. [Technology Stack](#technology-stack)
4. [Data Extraction & Ingestion](#data-extraction--ingestion)
5. [Data Normalization](#data-normalization)
6. [Data Validation & Quality Checks](#data-validation--quality-checks)
7. [Aggregation & Analytics](#aggregation--analytics)
8. [Report Generation](#report-generation)
9. [Web Interface](#web-interface)
10. [Challenges & Solutions](#challenges--solutions)
11. [Setup & Deployment](#setup--deployment)
12. [Future Enhancements](#future-enhancements)

---

## Project Architecture

### System Design

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     CLIENT BROWSER                          ‚îÇ
‚îÇ              React Frontend (Port 3001)                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ HTTP REST API
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  Flask Backend (Port 5000)                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ           RESTful API Endpoints                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ /api/clients  ‚Ä¢ /api/upload  ‚Ä¢ /api/process      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ /api/reports  ‚Ä¢ /api/jobs                        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              DATA PROCESSING PIPELINE                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇ Ingestion  ‚îÇ‚Üí ‚îÇNormalization‚îÇ‚Üí ‚îÇ Validation ‚îÇ           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ         ‚îÇ                                  ‚îÇ                ‚îÇ
‚îÇ         ‚ñº                                  ‚ñº                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇAggregation ‚îÇ                    ‚îÇ  Reports   ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FILE SYSTEM                              ‚îÇ
‚îÇ  data/           reports/           temp_uploads/           ‚îÇ
‚îÇ  ‚îú‚îÄ C001/        ‚îú‚îÄ C001_portfolio_report.xlsx            ‚îÇ
‚îÇ  ‚îú‚îÄ C002/        ‚îú‚îÄ C002_portfolio_report.xlsx            ‚îÇ
‚îÇ  ‚îî‚îÄ ...          ‚îî‚îÄ ...                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Directory Structure

```
portfolio-analytics/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îî‚îÄ‚îÄ main.py                    # FastAPI backend (deprecated)
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dashboard.jsx      # Client overview & report generation
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Upload.jsx         # File upload interface
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Reports.jsx        # Report download interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.jsx                # Main React app
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.jsx               # Entry point
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ vite.config.js
‚îú‚îÄ‚îÄ src/                           # Core processing modules
‚îÇ   ‚îú‚îÄ‚îÄ ingestion.py              # Data extraction from Excel files
‚îÇ   ‚îú‚îÄ‚îÄ normalizer.py             # Schema normalization
‚îÇ   ‚îú‚îÄ‚îÄ validator.py              # Data quality validation
‚îÇ   ‚îú‚îÄ‚îÄ aggregator.py             # Metric computation
‚îÇ   ‚îú‚îÄ‚îÄ report_generator.py       # Excel report creation
‚îÇ   ‚îú‚îÄ‚îÄ decimal_utils.py          # Precise decimal arithmetic
‚îÇ   ‚îî‚îÄ‚îÄ main.py                   # Pipeline orchestrator
‚îú‚îÄ‚îÄ data/                         # Client data storage
‚îÇ   ‚îú‚îÄ‚îÄ C001/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Uploaded_Files/
‚îÇ   ‚îú‚îÄ‚îÄ C002/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ reports/                      # Generated Excel reports
‚îú‚îÄ‚îÄ flask_backend.py             # Production Flask server
‚îú‚îÄ‚îÄ venv311/                     # Python 3.11 virtual environment
‚îî‚îÄ‚îÄ requirements.txt             # Python dependencies
```

---

## Data Pipeline Overview

### 5-Stage Processing Pipeline

```
[1] INGESTION ‚Üí [2] NORMALIZATION ‚Üí [3] VALIDATION ‚Üí [4] AGGREGATION ‚Üí [5] REPORTING
```

Each stage is designed to be:
- **Modular**: Can be tested independently
- **Robust**: Handles errors gracefully
- **Traceable**: Comprehensive logging
- **Scalable**: Processes multiple clients simultaneously

---

## Technology Stack

### Backend Technologies

| Technology | Version | Purpose | Why We Chose It |
|------------|---------|---------|-----------------|
| **Python** | 3.11.0 | Core language | Excellent data processing libraries, stability |
| **Flask** | 2.3+ | Web framework | Simple, lightweight, stable on Windows |
| **Flask-CORS** | 4.0+ | Cross-origin requests | Enable frontend-backend communication |
| **Pandas** | 2.0+ | Data manipulation | Industry standard for tabular data |
| **OpenPyXL** | 3.1+ | Excel file I/O | Read/write .xlsx files |
| **XlsxWriter** | 3.1+ | Excel report generation | Advanced formatting capabilities |
| **Pandera** | 0.17+ | Data validation | Schema-based validation framework |
| **Decimal** | Built-in | Precise arithmetic | Avoid floating-point errors in financial calculations |

**Note**: We initially tried FastAPI with uvicorn but encountered critical Windows compatibility issues (asyncio bugs in Python 3.11+). Flask's synchronous WSGI architecture proved more stable for Windows environments.

### Frontend Technologies

| Technology | Version | Purpose | Why We Chose It |
|------------|---------|---------|-----------------|
| **React** | 18.2+ | UI framework | Component-based, modern, widely supported |
| **Vite** | 5.0+ | Build tool | Fast dev server, hot module replacement |
| **Ant Design** | 5.12+ | UI components | Professional components, minimal custom CSS |
| **Axios** | 1.6+ | HTTP client | Promise-based, easy error handling |
| **React Router** | 6.20+ | Navigation | Client-side routing |
| **Day.js** | 1.11+ | Date formatting | Lightweight alternative to Moment.js |

---

## Data Extraction & Ingestion

### Problem Statement

Investment brokers export data in various inconsistent formats:
- Different column names
- Tab-separated values stored in single Excel columns
- Mixed date formats
- Inconsistent numerical representations
- Multiple file types (trade books, capital gains, holdings)

### Solution: Intelligent File Detection & Parsing

**File**: [`src/ingestion.py`](src/ingestion.py)

#### Key Features

1. **Automatic File Type Detection**
```python
def detect_file_type(file_path: str) -> Optional[str]:
    """
    Detects file type from filename patterns:
    - trade_book: tradebook, trades, trade_book
    - capital_gains: capital gains, capital_gains, capgain, cg
    - holdings: holding, holdings
    """
```

2. **Tab-Separated Data Extraction**
```python
def read_excel_with_tab_detection(file_path: str) -> pd.DataFrame:
    """
    Many brokers export data with all columns in a single cell,
    separated by tabs. This function:
    1. Reads the Excel file
    2. Detects if data is tab-separated
    3. Splits into proper columns
    4. Returns clean DataFrame
    """
```

3. **Client & Broker Extraction from Path**
```python
def extract_client_broker_from_path(file_path: str) -> Tuple[str, str]:
    """
    Extracts metadata from directory structure:
    .../C001/Broker_Name/file.xlsx
    Returns: ('C001', 'Broker_Name')
    """
```

#### Supported Brokers

- Charles Schwab
- Fidelity
- Groww
- HDFC Bank
- ICICI Direct
- Zerodha

#### Data Flow

```
Excel Files ‚Üí detect_file_type() ‚Üí read_excel_with_tab_detection() ‚Üí 
‚Üí extract_client_broker_from_path() ‚Üí Raw DataFrames
```

#### Error Handling

- **Invalid files**: Logged and skipped
- **Missing columns**: Warning logged, file marked for review
- **Encoding issues**: Multiple encoding attempts (utf-8, latin-1, cp1252)

---

## Data Normalization

### Problem Statement

Each broker uses different column names and formats for the same data:

| Broker | Date Column | Symbol Column | Quantity Column |
|--------|-------------|---------------|-----------------|
| Zerodha | Trade Date | Symbol | Qty. |
| Groww | Date | Ticker | Quantity |
| ICICI | Txn Date | Scrip | Vol |

### Solution: Canonical Schema Mapping

**File**: [`src/normalizer.py`](src/normalizer.py)

#### Canonical Schemas

**Trade Book Schema:**
```python
{
    'date': datetime,
    'symbol': str,
    'action': str,           # 'Buy' or 'Sell'
    'qty': Decimal,
    'price': Decimal,
    'amount': Decimal,
    'broker_name': str,
    'client_id': str
}
```

**Capital Gains Schema:**
```python
{
    'symbol': str,
    'buy_date': datetime,
    'sell_date': datetime,
    'quantity': Decimal,
    'buy_price': Decimal,
    'sell_price': Decimal,
    'gain_loss': Decimal,
    'gain_type': str,        # 'Short Term' or 'Long Term'
    'client_id': str,
    'broker_name': str
}
```

#### Broker-Specific Mappings

**Example - Zerodha Mapping:**
```python
ZERODHA_TRADE_MAPPING = {
    'Trade Date': 'date',
    'Symbol': 'symbol',
    'Action': 'action',
    'Qty.': 'qty',
    'Avg. Price': 'price',
    'Net Amount': 'amount'
}
```

#### Normalization Process

```
Raw DataFrame ‚Üí 
‚Üí detect_broker() ‚Üí 
‚Üí apply_mapping() ‚Üí 
‚Üí standardize_dates() ‚Üí 
‚Üí convert_to_decimal() ‚Üí 
‚Üí validate_schema() ‚Üí 
‚Üí Normalized DataFrame
```

#### Data Type Conversions

**Decimal Precision for Financial Data:**
```python
from decimal import Decimal, getcontext
getcontext().prec = 28  # High precision for financial calculations

def to_decimal(value):
    """Safely convert values to Decimal, handling errors"""
    try:
        return Decimal(str(value))
    except:
        return Decimal('0')
```

**Date Standardization:**
```python
def parse_date(date_str):
    """
    Handles multiple date formats:
    - DD/MM/YYYY
    - MM/DD/YYYY
    - YYYY-MM-DD
    - DD-MMM-YYYY
    """
    formats = ['%d/%m/%Y', '%m/%d/%Y', '%Y-%m-%d', '%d-%b-%Y']
    for fmt in formats:
        try:
            return datetime.strptime(date_str, fmt)
        except:
            continue
    raise ValueError(f"Cannot parse date: {date_str}")
```

---

## Data Validation & Quality Checks

### Problem Statement

Input data may contain:
- Missing required fields
- Negative quantities/prices
- Invalid date ranges
- Duplicate transactions
- Inconsistent symbols

### Solution: Schema-Based Validation with Pandera

**File**: [`src/validator.py`](src/validator.py)

#### Validation Rules

**Trade Book Validation:**
```python
trade_schema = pa.DataFrameSchema({
    "date": pa.Column(pa.DateTime, nullable=False),
    "symbol": pa.Column(pa.String, nullable=False, 
                        checks=pa.Check.str_length(min_value=1)),
    "qty": pa.Column(pa.Decimal, 
                     checks=pa.Check.greater_than(0)),
    "price": pa.Column(pa.Decimal, 
                       checks=pa.Check.greater_than(0)),
    "amount": pa.Column(pa.Decimal),
    "action": pa.Column(pa.String, 
                        checks=pa.Check.isin(['Buy', 'Sell']))
})
```

**Capital Gains Validation:**
```python
capital_gains_schema = pa.DataFrameSchema({
    "buy_date": pa.Column(pa.DateTime, nullable=False),
    "sell_date": pa.Column(pa.DateTime, nullable=False,
                          checks=pa.Check(lambda x, df: x >= df['buy_date'])),
    "quantity": pa.Column(pa.Decimal,
                         checks=pa.Check.greater_than(0)),
    "gain_type": pa.Column(pa.String,
                          checks=pa.Check.isin(['Short Term', 'Long Term']))
})
```

#### Validation Process

```
Normalized Data ‚Üí 
‚Üí schema.validate() ‚Üí 
‚Üí collect_errors() ‚Üí 
‚Üí log_warnings() ‚Üí 
‚Üí generate_error_report() ‚Üí 
‚Üí Validated Data + Error Report
```

#### Error Handling Strategy

- **Critical Errors**: Stop processing (missing client_id, malformed data)
- **Warnings**: Log and continue (minor inconsistencies)
- **Data Quality Report**: Included in final Excel report

---

## Aggregation & Analytics

### Computed Metrics

**File**: [`src/aggregator.py`](src/aggregator.py)

#### Client-Level Aggregations

1. **Portfolio Value**
   - Current holdings √ó latest market price
   - Unrealized gains/losses

2. **Realized Gains**
   - Short-term capital gains (< 1 year)
   - Long-term capital gains (‚â• 1 year)

3. **Transaction Statistics**
   - Total number of trades
   - Buy vs Sell volume
   - Average transaction size

4. **Broker Distribution**
   - Assets per broker
   - Transaction count per broker

5. **Symbol-Level Analytics**
   - Top holdings by value
   - Most traded symbols
   - Symbol-wise P&L

#### Aggregation Logic

```python
def compute_aggregated_metrics(trades_df, capital_gains_df, client_id):
    """
    Computes all portfolio metrics for a client:
    1. Current holdings (buy - sell)
    2. Realized gains from capital gains data
    3. Unrealized gains from open positions
    4. Broker-wise breakdowns
    5. Performance metrics
    """
    
    # Calculate current holdings
    holdings = trades_df.groupby(['symbol', 'broker_name']).agg({
        'qty': lambda x: sum_decimals(*x[x.action == 'Buy']) - 
                        sum_decimals(*x[x.action == 'Sell']),
        'amount': 'sum'
    })
    
    # Calculate realized gains
    realized_gains = capital_gains_df.groupby('gain_type')['gain_loss'].sum()
    
    return {
        'holdings': holdings,
        'realized_gains': realized_gains,
        'total_trades': len(trades_df),
        'portfolio_value': calculate_portfolio_value(holdings)
    }
```

---

## Report Generation

### Excel Report Structure

**File**: [`src/report_generator.py`](src/report_generator.py)

#### Multi-Sheet Report Layout

```
Portfolio Report (Excel)
‚îú‚îÄ‚îÄ üìä Summary
‚îÇ   ‚îú‚îÄ‚îÄ Client Information
‚îÇ   ‚îú‚îÄ‚îÄ Portfolio Overview
‚îÇ   ‚îú‚îÄ‚îÄ Performance Metrics
‚îÇ   ‚îî‚îÄ‚îÄ Key Statistics
‚îú‚îÄ‚îÄ üìà Current Holdings
‚îÇ   ‚îú‚îÄ‚îÄ Symbol | Quantity | Avg Price | Current Value | Unrealized P&L
‚îÇ   ‚îî‚îÄ‚îÄ Sorted by value (descending)
‚îú‚îÄ‚îÄ üí∞ Capital Gains
‚îÇ   ‚îú‚îÄ‚îÄ Short Term Gains
‚îÇ   ‚îî‚îÄ‚îÄ Long Term Gains
‚îú‚îÄ‚îÄ üìù Transaction History
‚îÇ   ‚îú‚îÄ‚îÄ All trades (chronological)
‚îÇ   ‚îî‚îÄ‚îÄ Filters: Date, Symbol, Action, Broker
‚îú‚îÄ‚îÄ üè¶ Broker Summary
‚îÇ   ‚îú‚îÄ‚îÄ Assets per broker
‚îÇ   ‚îî‚îÄ‚îÄ Transaction statistics
‚îî‚îÄ‚îÄ ‚ö†Ô∏è Data Quality Issues
    ‚îî‚îÄ‚îÄ Validation errors and warnings
```

#### Report Generation Process

```python
def generate_all_reports(trades_df, capital_gains_df, output_dir):
    """
    Generates comprehensive Excel reports for all clients
    """
    clients = get_all_clients(trades_df, capital_gains_df)
    
    for client_id in clients:
        # Filter data for client
        client_trades = trades_df[trades_df['client_id'] == client_id]
        client_cg = capital_gains_df[capital_gains_df['client_id'] == client_id]
        
        # Compute metrics
        metrics = compute_aggregated_metrics(client_trades, client_cg, client_id)
        
        # Generate Excel with formatting
        workbook = xlsxwriter.Workbook(f'{output_dir}/{client_id}_portfolio_report.xlsx')
        
        # Create sheets
        create_summary_sheet(workbook, metrics)
        create_holdings_sheet(workbook, metrics['holdings'])
        create_capital_gains_sheet(workbook, client_cg)
        create_transaction_sheet(workbook, client_trades)
        create_broker_sheet(workbook, metrics['broker_stats'])
        create_quality_sheet(workbook, validation_errors)
        
        workbook.close()
```

#### Excel Formatting

- **Conditional formatting**: Gains in green, losses in red
- **Number formats**: Currency, percentages, dates
- **Auto-sizing**: Columns auto-fit content
- **Freeze panes**: Headers remain visible when scrolling
- **Charts**: Portfolio distribution pie chart

---

## Web Interface

### Backend API (Flask)

**File**: [`flask_backend.py`](flask_backend.py)

#### REST API Endpoints

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/` | GET | Health check |
| `/api/clients` | GET | List all clients with status |
| `/api/upload` | POST | Upload data files for a client |
| `/api/process/{client_id}` | POST | Trigger report generation |
| `/api/jobs/{job_id}` | GET | Check processing status |
| `/api/reports/{client_id}` | GET | Download Excel report |
| `/api/clients/{client_id}` | DELETE | Delete client data |

#### Example: Upload Flow

```python
@app.route('/api/upload', methods=['POST'])
def upload_files():
    """
    Handles multi-file upload:
    1. Validate client_id format
    2. Create directory structure: data/{client_id}/Uploaded_Files/
    3. Save files with secure filenames
    4. Return success response with file metadata
    """
    client_id = request.form.get('client_id')
    files = request.files.getlist('files')
    
    # Auto-format client ID (5 ‚Üí C005)
    if not client_id.startswith('C'):
        client_id = f"C{client_id.zfill(3)}"
    
    # Save files
    for file in files:
        filename = secure_filename(file.filename)
        file.save(DATA_DIR / client_id / 'Uploaded_Files' / filename)
    
    return jsonify({"success": True, "client_id": client_id})
```

#### Example: Report Generation Flow

```python
@app.route('/api/process/<client_id>', methods=['POST'])
def process_client(client_id):
    """
    Synchronous report generation:
    1. Create job ID for tracking
    2. Run complete data pipeline
    3. Update job status
    4. Return completion status
    """
    job_id = str(uuid.uuid4())
    jobs[job_id] = {"status": "processing", "client_id": client_id}
    
    try:
        success = run_pipeline(
            data_dir=str(DATA_DIR),
            output_dir=str(REPORTS_DIR),
            fail_on_validation=False
        )
        
        if success:
            jobs[job_id]["status"] = "completed"
        else:
            jobs[job_id]["status"] = "failed"
    
    except Exception as e:
        jobs[job_id]["status"] = "failed"
        jobs[job_id]["error"] = str(e)
    
    return jsonify(jobs[job_id])
```

### Frontend (React)

#### Dashboard Component

**File**: [`frontend/src/pages/Dashboard.jsx`](frontend/src/pages/Dashboard.jsx)

**Features:**
- Real-time client list with status indicators
- One-click report generation
- Job status polling (checks every 2 seconds)
- Download button for completed reports
- Client deletion with confirmation

**Key Code:**
```javascript
const handleProcessClient = async (clientId) => {
    const response = await axios.post(`${API_BASE_URL}/api/process/${clientId}`);
    const jobId = response.data.job_id;
    
    // Poll for completion
    const interval = setInterval(async () => {
        const status = await axios.get(`${API_BASE_URL}/api/jobs/${jobId}`);
        
        if (status.data.status === 'completed') {
            clearInterval(interval);
            message.success(`Report generated for ${clientId}`);
            fetchClients();
        }
    }, 2000);
};
```

#### Upload Component

**File**: [`frontend/src/pages/Upload.jsx`](frontend/src/pages/Upload.jsx)

**Features:**
- Drag-and-drop file upload
- Multi-file selection
- Client ID auto-formatting
- File type validation (.xlsx, .xls only)
- Upload progress indication
- Success notifications

**Key Code:**
```javascript
const uploadProps = {
    multiple: true,
    beforeUpload: (file) => {
        const isExcel = file.name.endsWith('.xlsx') || file.name.endsWith('.xls');
        if (!isExcel) {
            message.error(`${file.name} is not an Excel file`);
            return Upload.LIST_IGNORE;
        }
        return false; // Prevent auto-upload
    }
};
```

#### Reports Component

**File**: [`frontend/src/pages/Reports.jsx`](frontend/src/pages/Reports.jsx)

**Features:**
- Searchable client list
- Filter by report availability
- Sort by date, client ID, status
- One-click download with progress indication
- Report metadata (generation date, file size)

---

## Challenges & Solutions

### Challenge 1: Python Version Compatibility ‚ùå ‚Üí ‚úÖ

**Problem:**
- Python 3.14 has critical asyncio bugs on Windows
- FastAPI/uvicorn would start but immediately crash on connection attempts
- Error: `OSError: [WinError 10014] invalid pointer address`

**Attempted Solutions:**
1. ‚ùå uvicorn with different loop policies
2. ‚ùå hypercorn (ASGI server)
3. ‚ùå waitress (WSGI-to-ASGI bridge)
4. ‚ùå Python 3.11 with uvicorn (same issue!)

**Final Solution:** ‚úÖ
- Switched from FastAPI to **Flask** (synchronous WSGI)
- Python 3.11 + Flask = Stable on Windows
- Flask's werkzeug server works perfectly for development

**Lesson Learned:**
For Windows development, synchronous frameworks (Flask, Django) are more reliable than async frameworks (FastAPI, Sanic) due to Windows' different I/O completion model.

---

### Challenge 2: Windows Firewall Blocking ‚ùå ‚Üí ‚úÖ

**Problem:**
- Flask server started successfully
- `netstat` showed port LISTENING
- But all connection attempts failed: "Unable to connect"
- No requests appearing in Flask logs

**Root Cause:**
Windows Firewall was blocking Python.exe from accepting inbound connections, even on localhost (127.0.0.1).

**Solution:** ‚úÖ
```powershell
# Run as Administrator
New-NetFirewallRule -DisplayName "Python Dev Server" `
    -Direction Inbound `
    -Program "C:\Users\praja\AppData\Local\Programs\Python\Python311\python.exe" `
    -Action Allow `
    -Profile Any
```

**Alternative (temporary test):**
```powershell
# Temporarily disable firewall to test
Set-NetFirewallProfile -Profile Domain,Public,Private -Enabled False
```

---

### Challenge 3: Tab-Separated Data in Excel ‚ùå ‚Üí ‚úÖ

**Problem:**
Brokers export data with all columns in a SINGLE Excel column, separated by tabs:
```
| Column A                                        |
|------------------------------------------------|
| Date\tSymbol\tQty\tPrice\tAmount               |
| 01/01/2024\tAAPL\t10\t150.00\t1500.00         |
```

**Solution:** ‚úÖ
```python
def read_excel_with_tab_detection(file_path):
    df = pd.read_excel(file_path)
    
    # Check if first column contains tabs
    first_col = df.iloc[:, 0].astype(str)
    if first_col.str.contains('\t').any():
        # Split by tabs and reconstruct DataFrame
        split_data = first_col.str.split('\t', expand=True)
        df = pd.DataFrame(split_data.values[1:], columns=split_data.iloc[0])
    
    return df
```

---

### Challenge 4: Floating-Point Precision Errors ‚ùå ‚Üí ‚úÖ

**Problem:**
```python
>>> 0.1 + 0.2
0.30000000000000004

>>> 100.01 * 100
10000.999999999998
```

Financial calculations require **exact** precision.

**Solution:** ‚úÖ
Use Python's `Decimal` module throughout:
```python
from decimal import Decimal, getcontext
getcontext().prec = 28

# All financial operations use Decimal
amount = Decimal('100.01') * Decimal('100')
# Result: Decimal('10001.00')
```

**Utility Functions:**
```python
def multiply_decimal(a: Decimal, b: Decimal) -> Decimal:
    return (a * b).quantize(Decimal('0.01'))

def divide_decimal(a: Decimal, b: Decimal) -> Decimal:
    if b == 0:
        return Decimal('0')
    return (a / b).quantize(Decimal('0.01'))
```

---

### Challenge 5: Broker-Specific Column Names ‚ùå ‚Üí ‚úÖ

**Problem:**
Different brokers use completely different column headers:

| Data Field | Zerodha | Groww | ICICI Direct |
|------------|---------|-------|--------------|
| Date | Trade Date | Txn Date | Settlement Date |
| Symbol | Symbol | Scrip Code | Security Name |
| Quantity | Qty. | Volume | Qty |

**Solution:** ‚úÖ
Mapping dictionaries for each broker:
```python
BROKER_MAPPINGS = {
    'Zerodha': {
        'Trade Date': 'date',
        'Symbol': 'symbol',
        'Qty.': 'qty',
        'Avg. Price': 'price'
    },
    'Groww': {
        'Txn Date': 'date',
        'Scrip Code': 'symbol',
        'Volume': 'qty',
        'Price': 'price'
    }
    # ... more brokers
}
```

---

### Challenge 6: Report Filename Mismatch ‚ùå ‚Üí ‚úÖ

**Problem:**
- Data pipeline generated: `C001_portfolio_report.xlsx`
- Frontend expected: `C001_portfolio.xlsx`
- Reports showed as "Not Generated" even though they existed

**Solution:** ‚úÖ
Updated Flask backend to check both naming conventions:
```python
report_path = REPORTS_DIR / f"{client_id}_portfolio_report.xlsx"
if not report_path.exists():
    report_path = REPORTS_DIR / f"{client_id}_portfolio.xlsx"
```

---

### Challenge 7: CORS Errors ‚ùå ‚Üí ‚úÖ

**Problem:**
Frontend on port 3001, backend on port 5000 ‚Üí blocked by browser CORS policy

**Solution:** ‚úÖ
```python
from flask_cors import CORS
app = Flask(__name__)
CORS(app)  # Enable CORS for all routes
```

---

## Setup & Deployment

### Prerequisites

- Python 3.11 (NOT 3.14 due to Windows bugs)
- Node.js 16+
- Windows 10/11 with admin access

### Installation Steps

**1. Clone Repository**
```powershell
cd "C:\Users\praja\Desktop\demo investment project"
```

**2. Setup Python Environment**
```powershell
py -3.11 -m venv venv311
.\venv311\Scripts\Activate.ps1
pip install -r requirements.txt
pip install flask flask-cors
```

**3. Setup Frontend**
```powershell
cd frontend
npm install
```

**4. Configure Firewall**
```powershell
# Run as Administrator
New-NetFirewallRule -DisplayName "Python Dev Server" `
    -Direction Inbound `
    -Program "C:\Users\praja\AppData\Local\Programs\Python\Python311\python.exe" `
    -Action Allow -Profile Any
```

**5. Run Application**

Terminal 1 - Backend:
```powershell
.\venv311\Scripts\Activate.ps1
python flask_backend.py
```

Terminal 2 - Frontend:
```powershell
cd frontend
npm run dev
```

**6. Access Application**
```
http://localhost:3001
```

---

## Future Enhancements

### Phase 1: Security & Authentication
- [ ] User authentication (JWT)
- [ ] Role-based access control
- [ ] Client-specific user accounts
- [ ] Audit logging

### Phase 2: Advanced Analytics
- [ ] Real-time portfolio tracking
- [ ] Market data integration (live prices)
- [ ] Performance benchmarking (vs indices)
- [ ] Tax optimization suggestions
- [ ] Risk analysis (beta, volatility, Sharpe ratio)

### Phase 3: Visualization
- [ ] Interactive charts (portfolio distribution, performance over time)
- [ ] Comparison charts (multiple clients)
- [ ] Heat maps (sector allocation, broker distribution)

### Phase 4: Automation
- [ ] Scheduled report generation
- [ ] Email notifications
- [ ] Automatic data refresh from broker APIs
- [ ] Alert system (price targets, stop losses)

### Phase 5: Scale & Performance
- [ ] PostgreSQL database (replace file system)
- [ ] Redis caching (faster report access)
- [ ] Celery task queue (async processing)
- [ ] Docker containerization
- [ ] Cloud deployment (AWS/Azure/GCP)

### Phase 6: Mobile Support
- [ ] Responsive design for tablets
- [ ] Progressive Web App (PWA)
- [ ] Native mobile apps (React Native)

---

## Performance Metrics

### Current Performance

- **File Ingestion**: ~50 files in 2-3 seconds
- **Normalization**: 2258 trades + 1096 capital gains in 3 seconds
- **Validation**: <1 second for 3000+ records
- **Report Generation**: 4 clients in 6 seconds
- **Total Pipeline**: ~10 seconds for complete processing

### Scalability Tests

| Metric | Current | Target (Phase 5) |
|--------|---------|------------------|
| Clients | 4 | 1000+ |
| Transactions/Client | ~500 | 10,000+ |
| Concurrent Users | 1 | 100+ |
| Report Generation Time | 6s | <2s (with caching) |
| File Upload Size | 10MB | 100MB |

---

## Conclusion

This portfolio analytics platform successfully addresses the challenge of consolidating and analyzing multi-broker investment data through a robust 5-stage pipeline. Despite encountering Windows-specific technical challenges (Python 3.14 asyncio bugs, firewall issues), we delivered a production-ready solution using Flask + React with comprehensive data processing capabilities.

### Key Achievements

‚úÖ **Automated Data Processing**: 50+ files processed in seconds  
‚úÖ **Multi-Broker Support**: 6 brokers with extensible architecture  
‚úÖ **Financial Precision**: Decimal-based calculations (no rounding errors)  
‚úÖ **User-Friendly Interface**: Modern React UI with drag-and-drop upload  
‚úÖ **Comprehensive Reports**: 6-sheet Excel reports with formatting  
‚úÖ **Production-Ready**: Error handling, logging, validation  

### Technical Excellence

- Clean, modular architecture
- Comprehensive error handling
- Schema-based validation
- Extensive logging for debugging
- Type safety with Pandera
- Financial-grade precision with Decimal

---

## Appendix

### A. File Format Specifications

**Trade Book Expected Columns:**
- Date/Trade Date/Txn Date
- Symbol/Scrip/Security Name
- Action/Type (Buy/Sell)
- Quantity/Qty/Volume
- Price/Rate
- Amount/Value

**Capital Gains Expected Columns:**
- Symbol
- Buy Date/Purchase Date
- Sell Date/Sale Date
- Quantity
- Buy Price
- Sell Price
- Gain/Loss
- Term Type (Short/Long)

### B. Error Codes

| Code | Error | Solution |
|------|-------|----------|
| E001 | Invalid file format | Ensure .xlsx or .xls |
| E002 | Missing required columns | Check broker template |
| E003 | Invalid date format | Use DD/MM/YYYY |
| E004 | Negative quantity | Verify data export |
| E005 | Client not found | Check client ID format |

### C. API Response Codes

| Code | Meaning |
|------|---------|
| 200 | Success |
| 400 | Bad request (validation error) |
| 404 | Resource not found |
| 500 | Server error |

---

**Document Version:** 1.0  
**Last Updated:** December 23, 2025  
**Author:** Portfolio Analytics Team  
**Status:** Production Ready
